{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ELMo tfhub](https://tfhub.dev/google/elmo/2)\n",
    "> Embeddings from a language model trained on the 1 Billion Word Benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Overview](https://tfhub.dev/google/elmo/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Computes contextualized word representations using character-based word representations and bidirectional LSTMs, as described in the paper \"[Deep contextualized word representations](https://arxiv.org/abs/1802.05365v2)\".\n",
    "2. This modules supports inputs both in the form of raw text strings or tokenized text strings.\n",
    "\n",
    "3. The module outputs fixed embeddings at each LSTM layer, a learnable aggregation of the 3 layers, and a fixed mean-pooled vector representation of the input.\n",
    "\n",
    "4. The complex architecture achieves state of the art results on several benchmarks. Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups. The use of an accelerator is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "The module defines two signatures: default, and tokens.\n",
    "\n",
    "With the default signature, the module takes untokenized sentences as input. The input tensor is a string tensor with shape [batch_size]. The module tokenizes each string by splitting on spaces.\n",
    "\n",
    "With the tokens signature, the module takes tokenized sentences as input. The input tensor is a string tensor with shape [batch_size, max_length] and an int32 tensor with shape [batch_size] corresponding to the sentence length. The length input is necessary to exclude padding in the case of sentences with varying length.\n",
    "\n",
    "## Outputs\n",
    "The output dictionary contains:\n",
    "\n",
    "+ word_emb: the character-based word representations with shape [batch_size, max_length, 512].\n",
    "+ lstm_outputs1: the first LSTM hidden state with shape [batch_size, max_length, 1024].\n",
    "+ lstm_outputs2: the second LSTM hidden state with shape [batch_size, max_length, 1024].\n",
    "+ elmo: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape [batch_size, max_length, 1024]\n",
    "+ default: a fixed mean-pooling of all contextualized word representations with shape [batch_size, 1024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "elmo = hub.Module(\"/home/b418/jupyter_workspace/B418_common/袁宵/tfhub_modules/elmo\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the **trainable parameter** to **True** when creating the module so that the 4 scalar weights (as described in the paper) can be trained. In this setting, the module still keeps all other parameters fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage method 1\n",
    "> signature=\"default\" as_dict=Flase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "Tensor(\"module_apply_default/truediv:0\", shape=(2, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embeddings = elmo(inputs=[\"the cat is on the mat\", \"dogs are in the fog\"],as_dict=False,signature=\"default\")\n",
    "#shape=(batch_size, 1024), dtype=float32\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage method 2\n",
    "> signature=\"default\" as_dict=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "module_features = elmo(inputs=[\"the cat is on the mat\", \"dogs are in the fog\"],as_dict=True,signature=\"default\")\n",
    "elmo_embedding = module_features[\"elmo\"]  #[batch_size, max_length, 1024], the weighted sum of the 3 layers, where the weights are trainable.\n",
    "word_emb = module_features[\"word_emb\"] #[batch_size, max_length, 512], the character-based word representations\n",
    "lstm_outputs1 = module_features[\"lstm_outputs1\"] #[batch_size, max_length, 1024], the first LSTM hidden state\n",
    "lstm_outputs2 = module_features[\"lstm_outputs2\"] #[batch_size, max_length, 1024], the second LSTM hidden state\n",
    "default = module_features[\"default\"] #[batch_size, 1024], a fixed mean-pooling of all contextualized word representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"module_apply_default_1/aggregation/mul_3:0\", shape=(2, 6, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(elmo_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage method 3\n",
    "> signature=\"tokens\" as_dict=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
    "[\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\n",
    "tokens_length = [6, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "Tensor(\"module_apply_tokens/truediv:0\", shape=(2, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embeddings = elmo(inputs={\"tokens\":tokens_input, \"sequence_len\": tokens_length}, as_dict=False,signature=\"tokens\")\n",
    "#shape=(batch_size, 1024), dtype=float32\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage method 4\n",
    "> signature=\"tokens\" as_dict=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
    "[\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\n",
    "tokens_length = [6, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "module_features = elmo(inputs={\"tokens\":tokens_input, \"sequence_len\": tokens_length}, as_dict=True, signature=\"tokens\")\n",
    "elmo_embedding = module_features[\"elmo\"]  #[batch_size, max_length, 1024], the weighted sum of the 3 layers, where the weights are trainable.\n",
    "word_emb = module_features[\"word_emb\"] #[batch_size, max_length, 512], the character-based word representations\n",
    "lstm_outputs1 = module_features[\"lstm_outputs1\"] #[batch_size, max_length, 1024], the first LSTM hidden state\n",
    "lstm_outputs2 = module_features[\"lstm_outputs2\"] #[batch_size, max_length, 1024], the second LSTM hidden state\n",
    "default = module_features[\"default\"] #[batch_size, 1024], a fixed mean-pooling of all contextualized word representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"module_apply_tokens_1/aggregation/mul_3:0\", shape=(2, 6, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(elmo_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
