{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# elmo_word_level_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example use https://tfhub.dev/google/elmo/2\n",
    "\n",
    "Text embeddings can be performed at individual word level present inside the sentence or can be performed at the entire sentence level. \n",
    "\n",
    "**This paper deals with word-level embedding**.\n",
    "\n",
    "\n",
    "```\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "tokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
    "[\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\n",
    "tokens_length = [6, 5]\n",
    "embeddings = elmo(\n",
    "inputs={\n",
    "\"tokens\": tokens_input,\n",
    "\"sequence_len\": tokens_length\n",
    "},\n",
    "signature=\"tokens\",\n",
    "as_dict=True)[\"elmo\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dead work\n",
    "1. Download the data from [Kaggle's movie review sentiment analysis](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)\n",
    "2. Unzip the downloaded files,store train.tsv and test.tsv to data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, params):\n",
    "        self._train_file = os.path.abspath(params['TRAIN_FILE'])\n",
    "        self._test_file = os.path.abspath(params['TEST_FILE'])\n",
    "        self._train_val_split = params['TRAIN_VAL_RATIO']\n",
    "        self._max_sentence_words = params['MAX_SENTENCE_WORDS']\n",
    "\n",
    "        self._fetch_data()\n",
    "        self._process_data()\n",
    "\n",
    "    def _fetch_data(self):\n",
    "        X_data, y_data = self._read_train_data()\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X_data, y_data,\n",
    "                                                                        train_size=self._train_val_split)\n",
    "\n",
    "        self.X_test, self.X_test_id = self._read_test_data()\n",
    "\n",
    "    def _read_train_data(self):\n",
    "        pd_data = pd.read_csv(self._train_file, sep='\\t')\n",
    "        X_data, y_data = pd_data['Phrase'], pd_data['Sentiment']\n",
    "        return X_data, y_data\n",
    "\n",
    "    def _read_test_data(self):\n",
    "        pd_data = pd.read_csv(self._test_file, sep='\\t')\n",
    "        X_data, X_data_id = pd_data['Phrase'], pd_data['PhraseId']\n",
    "        return X_data, X_data_id\n",
    "\n",
    "    def _process_data(self):\n",
    "        self.X_train, self.train_seq_len = self._process_text_sequence(self.X_train)\n",
    "        self.X_val, self.val_seq_len = self._process_text_sequence(self.X_val)\n",
    "        self.X_test, self.test_seq_len = self._process_text_sequence(self.X_test)\n",
    "\n",
    "    # Doing this in Data preprocessing and not in Iterator as I wasn't able to see much support in tf.String\n",
    "    def _process_text_sequence(self, sentences):\n",
    "        tokenized_sentences = []\n",
    "        sequence_len = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(' ')\n",
    "            sequence_len.append(min(len(tokens), self._max_sentence_words))\n",
    "            if len(tokens) > self._max_sentence_words:\n",
    "                tokens = tokens[:self._max_sentence_words]\n",
    "            else:\n",
    "                tokens = tokens + [''] * (self._max_sentence_words - len(tokens))\n",
    "            tokenized_sentences.append(tokens)\n",
    "        return tokenized_sentences, sequence_len\n",
    "\n",
    "    def get_train_data_length(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def get_val_data_length(self):\n",
    "        return len(self.y_val)\n",
    "\n",
    "    def get_test_data_length(self):\n",
    "        return len(self.X_test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, params):\n",
    "        self._n_class = params['N_CLASS']\n",
    "        self._batch_size = params['BATCH_SIZE']\n",
    "\n",
    "        self._create_iterator()\n",
    "\n",
    "    def _create_iterator(self):\n",
    "        self._pl_phrase_id = tf.placeholder(tf.int32, (None), name='pl_phrase_id')\n",
    "        self._pl_phrase_text = tf.placeholder(tf.string, (None), name='pl_phrase_text')\n",
    "        self._pl_phrase_len = tf.placeholder(tf.int32, (None), name='pl_phrase_len')\n",
    "        self._pl_sentiment = tf.placeholder(tf.int32, (None), name='pl_sentiment')\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self._pl_phrase_id, self._pl_phrase_text,\n",
    "                                                      self._pl_phrase_len, self._pl_sentiment))\n",
    "        dataset = dataset.batch(self._batch_size)\n",
    "\n",
    "        iterator = tf.data.Iterator.from_structure(dataset.output_types)\n",
    "        self._iterator_initializer = iterator.make_initializer(dataset, name='initializer')\n",
    "        self.text_id, self.text_data, self.text_len, self.labels = iterator.get_next()\n",
    "        self.text_id = tf.identity(self.text_id, name='text_id')\n",
    "\n",
    "    def initialize_iterator(self, sess, phrase_text, phrase_len, sentiment):\n",
    "        phrase_id = np.zeros((len(phrase_text)), dtype=np.int32)\n",
    "        feed_dict = {\n",
    "            self._pl_phrase_id: phrase_id,\n",
    "            self._pl_phrase_text: phrase_text,\n",
    "            self._pl_phrase_len: phrase_len,\n",
    "            self._pl_sentiment: sentiment\n",
    "        }\n",
    "        sess.run(self._iterator_initializer, feed_dict=feed_dict)\n",
    "\n",
    "    def initialize_test_iterator_for_saved_model_graph(self, sess, phrase_id, phrase_text, phrase_len):\n",
    "        pl_phrase_id = sess.graph.get_tensor_by_name('pl_phrase_id:0')\n",
    "        pl_phrase_text = sess.graph.get_tensor_by_name('pl_phrase_text:0')\n",
    "        pl_phrase_len = sess.graph.get_tensor_by_name('pl_phrase_len:0')\n",
    "        pl_sentiment = sess.graph.get_tensor_by_name('pl_sentiment:0')\n",
    "        initializer = sess.graph.get_operation_by_name('initializer')\n",
    "\n",
    "        sentiment = np.zeros((len(phrase_text)), dtype=np.float32)\n",
    "        feed_dict = {\n",
    "            pl_phrase_id: phrase_id,\n",
    "            pl_phrase_text: phrase_text,\n",
    "            pl_phrase_len: phrase_len,\n",
    "            pl_sentiment: sentiment\n",
    "        }\n",
    "        sess.run(initializer, feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, params):\n",
    "        self._n_class = params['N_CLASS']\n",
    "        self._max_sentence_words = params['MAX_SENTENCE_WORDS']\n",
    "        self._prepare_graph(params)\n",
    "\n",
    "    def _prepare_graph(self, params):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.dataset = Dataset(params)\n",
    "\n",
    "        self.lr = tf.placeholder(tf.float32, shape=())\n",
    "        one_hot_y = tf.one_hot(self.dataset.labels, depth=self._n_class)\n",
    "        logits = self._prepare_model(self.dataset.text_data, self.dataset.text_len)\n",
    "\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot_y)\n",
    "        self.loss = tf.reduce_sum(cross_entropy)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "        self.predictions = tf.argmax(logits, axis=1, output_type=tf.int32, name='predictions')\n",
    "        self.accuracy = tf.reduce_sum(tf.cast(tf.equal(self.predictions, self.dataset.labels), tf.float32))\n",
    "\n",
    "    def _prepare_model(self, tokens, sequence_len):\n",
    "        \"\"\"\n",
    "        :param tokens: [batch_size, max_length]\n",
    "        :param sequence_len:[batch_size,]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # elmo_url=\"https://tfhub.dev/google/elmo/2\"\n",
    "        # hub.Module(elmo_url, trainable=True)\n",
    "        # You can either use the URL directly or download the file locally and then use it.\n",
    "        module = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "        module_features = module(dict(tokens=tokens, sequence_len=sequence_len),\n",
    "                                 signature='tokens', as_dict=True)\n",
    "        embeddings = module_features[\"elmo\"] #[batch_size, max_length, 1024]\n",
    "        #print(embeddings)\n",
    "\n",
    "        with tf.variable_scope('Layer1'):\n",
    "            cell_fw1 = tf.nn.rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\n",
    "            cell_bw1 = tf.nn.rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\n",
    "\n",
    "            outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=cell_fw1,\n",
    "                cell_bw=cell_bw1,\n",
    "                inputs=embeddings,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "        output_fw, output_bw = outputs\n",
    "        outputs_BiLSTM = tf.concat([output_fw, output_bw], axis=-1)\n",
    "        rnn_output = tf.reshape(outputs_BiLSTM, (-1, 2 * 128 * self._max_sentence_words))\n",
    "        with tf.variable_scope('Layer2'):\n",
    "            weight2 = tf.get_variable('weight', initializer=tf.truncated_normal((2 * 128 * self._max_sentence_words,\n",
    "                                                                                 self._n_class)))\n",
    "            bias2 = tf.get_variable('bias', initializer=tf.ones(self._n_class))\n",
    "            logits = tf.nn.xw_plus_b(rnn_output, weight2, bias2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, params):\n",
    "        self._epochs = params['EPOCHS']\n",
    "        self._batch_size = params['BATCH_SIZE']\n",
    "        self._lr = params['LEARNING_RATE']\n",
    "        self._n_class = params['N_CLASS']\n",
    "        self._divide_lr = params['DIVIDE_LEARNING_RATE_AT']\n",
    "\n",
    "        self.data = Data(params)\n",
    "        self.model = Model(params)\n",
    "\n",
    "        self._save_path = os.path.abspath('./Model_OUTPUT(word_level_elmo)')\n",
    "\n",
    "    def train(self):\n",
    "        shutil.rmtree(self._save_path, ignore_errors=True)\n",
    "        os.mkdir(self._save_path)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            current_lr = self._lr\n",
    "\n",
    "            for epoch_no in range(self._epochs):\n",
    "                train_loss, train_accuracy = 0, 0\n",
    "                val_loss, val_accuracy = 0, 0\n",
    "                if epoch_no in self._divide_lr:\n",
    "                    current_lr /= 10\n",
    "\n",
    "                print('\\nEpoch: {}, lr: {:.6f}'.format(epoch_no + 1, current_lr))\n",
    "                self.model.dataset.initialize_iterator(sess, self.data.X_train,\n",
    "                                                       self.data.train_seq_len, self.data.y_train)\n",
    "                try:\n",
    "                    with tqdm(total=self.data.get_train_data_length()) as pbar:\n",
    "                        while True:\n",
    "                            _, l, a = sess.run([self.model.optimizer, self.model.loss, self.model.accuracy],\n",
    "                                               feed_dict={self.model.lr: current_lr})\n",
    "                            train_loss += l\n",
    "                            train_accuracy += a\n",
    "                            pbar.update(self._batch_size)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "\n",
    "                self.model.dataset.initialize_iterator(sess, self.data.X_val,\n",
    "                                                       self.data.val_seq_len, self.data.y_val)\n",
    "                try:\n",
    "                    with tqdm(total=self.data.get_val_data_length()) as pbar:\n",
    "                        while True:\n",
    "                            l, a = sess.run([self.model.loss, self.model.accuracy])\n",
    "                            val_loss += l\n",
    "                            val_accuracy += a\n",
    "                            pbar.update(self._batch_size)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "\n",
    "                train_accuracy /= self.data.get_train_data_length()\n",
    "                train_loss /= self.data.get_train_data_length()\n",
    "                val_accuracy /= self.data.get_val_data_length()\n",
    "                val_loss /= self.data.get_val_data_length()\n",
    "\n",
    "                print('Train accuracy: {:.4f}, loss: {:.4f}'.format(train_accuracy, train_loss))\n",
    "                print('Validation accuracy: {:.4f}, loss: {:.4f}'.format(val_accuracy, val_loss))\n",
    "                self._save_model(sess, epoch_no)\n",
    "\n",
    "    def test(self):\n",
    "        test_graph = tf.Graph()\n",
    "        with test_graph.as_default():\n",
    "            with tf.Session(graph=test_graph) as sess, \\\n",
    "                    open(os.path.join(self._save_path, 'results.csv'), 'w') as fid:\n",
    "                sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "                saved_model_path = os.path.join(self._save_path, str(self._epochs - 1))\n",
    "                tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)\n",
    "                self.model.dataset.initialize_test_iterator_for_saved_model_graph(sess, self.data.X_test_id,\n",
    "                                                                    self.data.X_test, self.data.test_seq_len)\n",
    "\n",
    "                csv_fid = csv.writer(fid)\n",
    "                csv_fid.writerow(['PhraseId', 'Sentiment'])\n",
    "\n",
    "                predictions_op = test_graph.get_tensor_by_name('predictions:0')\n",
    "                text_id_op = test_graph.get_tensor_by_name('text_id:0')\n",
    "                try:\n",
    "                    with tqdm(total=self.data.get_test_data_length()) as pbar:\n",
    "                        while True:\n",
    "                            predictions, phrase_id = sess.run([predictions_op, text_id_op])\n",
    "                            predictions = predictions.tolist()\n",
    "                            phrase_id = phrase_id.tolist()\n",
    "\n",
    "                            for pred, p in zip(predictions, phrase_id):\n",
    "                                csv_fid.writerow([p, pred])\n",
    "                            pbar.update(self._batch_size)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    pass\n",
    "\n",
    "    def _save_model(self, sess, epoch_no):\n",
    "        inputs = {\n",
    "            'pl_phrase_id': sess.graph.get_tensor_by_name('pl_phrase_id:0'),\n",
    "            'pl_phrase_text': sess.graph.get_tensor_by_name('pl_phrase_text:0'),\n",
    "            'pl_phrase_len': sess.graph.get_tensor_by_name('pl_phrase_len:0'),\n",
    "            'pl_sentiment': sess.graph.get_tensor_by_name('pl_sentiment:0')\n",
    "        }\n",
    "        outputs = {'accuracy': self.model.accuracy}\n",
    "\n",
    "        export_dir = os.path.join(self._save_path, str(epoch_no))\n",
    "        tf.saved_model.simple_save(sess, export_dir, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b418/anaconda3/envs/yuanxiao/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "\n",
      "Epoch: 1, lr: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 44032/148257 [2:24:44<6:09:12,  4.70it/s]"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'EPOCHS': 10,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'DIVIDE_LEARNING_RATE_AT': [5],      # Where the learning rate should be divided at. Count starts from 0\n",
    "\n",
    "    'TRAIN_FILE': './Data/train.tsv',\n",
    "    'TEST_FILE': './Data/test.tsv',\n",
    "    'TRAIN_VAL_RATIO': 0.95,\n",
    "    'MAX_SENTENCE_WORDS': 25,   # How many words in sentence is to be considered.\n",
    "\n",
    "    'N_CLASS': 5\n",
    "}\n",
    "\n",
    "t = Train(params)\n",
    "t.train()\n",
    "t.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
